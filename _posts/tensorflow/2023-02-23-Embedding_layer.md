---
title: "Embedding Layer란 무엇일까?"
toc: true
toc_sticky: true
categories: Tensorflow
---


### 정의

임베딩 레이어는 단어 입력에 대해서 학습이 필요할 때, 단어를 의미론적 기하공간에 매핑할 수 있도록 벡터화 시키는 레이어이다.  
유사한 맥락에서 나타나는 단어는 그 의미도 비슷하다는 분포 가설에 따라, 모든 단어를 분산 표현하기 위해서 벡터화 시킨다는 얘기이다.  
유사한 맥락에 나타난 단어들끼리는 두 단어 벡터 사이의 거리를 가깝게 하고, 그렇지 않는 단어들끼리는 멀어지도록 조금씩 조정한다.  
만약 단어 단위가 아니라 문자 단위를 입출력으로 사용한다면 굳이 의미를 찾지 않아도 되기 때문에 임베딩 레이어를 사용하지 않는다.  

쉽게 말하면, 단어 단위 시퀀스가 입력으로 들어왔을 때, 단어의 의미를 벡터로 표현할 수 있게 하는 레이어가 임베딩 레이어라고 생각하면 된다.
> 단어의 특징과 유사도를 나타내주는 진정한 임베딩은 Word2Vec과 같은 학습을 통한 방법이 있다.

### 사용방법

<span style="color:#FCF2CE">***Hope to see you soon***</span>  
<span style="color:#FCF2CE">***Nice to see you again***</span>  
이라는 문장이 있다고 하자.  
각 단어를 토큰화 시켜서 정수 인코딩을 한 결과는  
[0, 1, 2, 3, 4]  
[5, 1, 2, 3, 6]  
이라고 하자.  
임베딩 레이어를 사용할 때, 첫번째 인수로 어휘의 크기를 지정해줘야 한다. 그 이유는 가능한 모든 단어를 포함하는 벡터를 만들기 위해서이다. 예시에서는 어휘의 크기는 Hope, to, see, you, soon, Nice, again 7개가 있다.  
두번째 인수로는 임베딩 벡터의 크기를 나타낸다. 코드로 예시를 보자


```python
from tensorflow.keras.layers import Embedding
Embedding(7, 2, input_length=5) # input_length는 시퀀스의 크기이다
```

위 코드의 뜻은 총 7개의 단어를 2차원으로 분산 표현 한다는 뜻이다. 즉, 입력으로 들어온 원-핫 벡터값은 각 인덱스가 가르키는 임베딩 레이어의 분산표현을 나타낸다. 레이어가 생성되면 가중치 행렬이 초기화 되는데 여기서는 (7, 2)크기의 행렬이 초기화 된다. 물론 처음으로 초기화된 값은 아무런 쓸모가 없다. 이 행렬은 학습을 통해 최적의 값을 찾게 되고, 그 의미는 단어의 유사도에 따라 잘 매핑되었다(?) 라는 뜻이다.  

주의할 점은 Embedding Layer는 원-핫 벡터에 맞게 그저 대응을 시켜줄 뿐이므로 미분이라는 것이 없다. 그러므로 활성화 함수가 없는 그냥 linear한 형태로 계산만 되는 것이다. 그리고 Embedding Layer는 입력에 직접 연결되게 사용해야 한다. 어떤 연산 결과를 임베딩 레이어에 연결시키는 것은 불가능하다는 뜻이다.

### 오늘의 정리

진정한 임베딩이 아니라 분포 가설에 따라 비슷한 맥락의 단어들끼리 벡터공간에 매핑시키는 것이다.
